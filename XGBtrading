rm(list = ls())

getwd()
setwd('C:/Users/Matt/Desktop/PhD/xgboost_trading')

library(quantmod)
library(TTR)
library(xgboost)

getSymbols('GOOG', from = "2013-01-01", to = "2017-06-01", src = "yahoo")
IBEX <- GOOG
colnames(IBEX) = c("IBEX.Open","IBEX.High","IBEX.Low","IBEX.Close", "IBEX.Volumne", "IBEX.Adjusted")

IBEX <- na.omit(IBEX) #Removed a number of values, do something about this

# Define the technical indicators to build the model 
rsi <- RSI(IBEX$IBEX.Close, n=14, maType="WMA")
adx <- data.frame(ADX(IBEX[, c("IBEX.High", "IBEX.Low", "IBEX.Close")]))
sar <- SAR(IBEX[,c("IBEX.High","IBEX.Low")], accel = c(0.02, 0.2))
trend <- IBEX$IBEX.Close - sar


# create a lag in the technical indicators to avoid look-ahead bias 
rsi <- lag(rsi, 1)
adx$ADX <- lag(adx$ADX, 1)
trend <- lag(trend, 1)

# Create the target variable
price <- IBEX$IBEX.Close - IBEX$IBEX.Open
class <- ifelse(price > 0, 1, 0)

#Create the Matrix
model_df <- data.frame(class, rsi, adx$ADX, trend)

model <- na.omit(model_df)

#model <- matrix(c(class, rsi, adx$ADX, trend), nrow = length(class))
#model <- na.omit(model)
colnames(model) = c("class","rsi","adx","trend")

model <- as.matrix(model)


# Split data into train and test sets 
train_size = 2/3
breakpoint = nrow(model) * train_size

training_data = model[1:breakpoint,]
test_data = model[(breakpoint+1):nrow(model),]

# Split data training and test data into X and Y
X_train = training_data[,2:4] ; Y_train = training_data[,1]
class(X_train)[1]; class(Y_train)

X_test = test_data[,2:4] ; Y_test = test_data[,1]
class(X_test)[1]; class(Y_test)

# Train the xgboost model using the "xgboost" function
dtrain = xgb.DMatrix(data = X_train, label = Y_train)
xgModel = xgboost(data = dtrain, nround = 5, objective = "binary:logistic")

# Using cross validation
dtrain = xgb.DMatrix(data = X_train, label = Y_train)
cv = xgb.cv(data = dtrain, nround = 10, nfold = 5, objective = "binary:logistic")

# Make the predictions on the test data
preds = predict(xgModel, X_test)

# Determine the size of the prediction vector
print(length(preds))

# Limit display of predictions to the first 6
print(head(preds))

prediction = as.numeric(preds > 0.5)
print(head(prediction))

# Measuring performance
# using a simple metric, compare the predicted score
#with the threshold of 0.5
#e.g. if predicted score is less than 0.5 then the (preds > 0.5)
#expression gives a value of 0.
#If the value is not equal to the actual result from the test data set,
#then it is taken as a wrong result.

# Measuring model performance
error_value = mean(as.numeric(preds > 0.5) != Y_test)
print(paste("test-error=", error_value))
# 47% test error and the confusion matrix below gives us a value of 0.489

########################################3

upordown <- ifelse(preds > 0.5, 1, 0) # 1 market is up, 0 if the market is down
upordown

t <- table(predictions = upordown, actual = Y_test)
t
t_table <- as.data.frame(t)
t_table

# Calculating the accuracy
accuaracy <- (t_table[1, "Freq"] + t_table[2, "Freq"]) / (t_table[1, "Freq"] + t_table[2, "Freq"] + t_table[3, "Freq"] + t_table[4, "Freq"])
accuaracy
#(TP+TN)/(TP+TN+FP+FN)

#####################################

importance_matrix = xgb.importance(model = xgModel)
print(importance_matrix)


# View the trees from a model
xgb.plot.tree(model = xgModel)

# View only the first tree in the XGBoost model
xgb.plot.tree(model = xgModel, n_first_tree = 1)
